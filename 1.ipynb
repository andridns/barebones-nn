{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activations\n",
    "import layers\n",
    "import metrics\n",
    "import model\n",
    "from sklearn import datasets\n",
    "import preprocessing\n",
    "from utils import global_param_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = preprocessing.train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [4, 32, 64, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 173/3000 [00:00<00:12, 224.72it/s, loss=0.431, train_acc=0.942, val_acc=1]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping at Epoch: 186\n",
      "Training Finished\n",
      "Training Accuracy Score: 94.17%\n",
      "Validation Accuracy Score: 100.00%\n",
      "Training Logs are saved as a CSV file.\n",
      "Training Curves are saved as a PNG file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m = model.feedforward_neuralnet(X_train, y_train, X_test, y_test, layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params', 'losses', 'training_accuracy', 'validation_accuracy', 'layer_dims', 'activation', 'weight_init', 'dropout_rate', 'learning_rate', 'best_epoch'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layer_dims, \n",
    "               activation='relu', \n",
    "               weight_init='glorot_uniform', \n",
    "               dropout_rate=None, \n",
    "               learning_rate=0.0495, \n",
    "               num_steps=3000, \n",
    "               early_stopping=True):\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_steps = num_steps\n",
    "        self.early_stopping = early_stopping\n",
    "        self.params = global_param_init(layer_dims, weight_init)\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "    def plot(self):\n",
    "        pass\n",
    "    \n",
    "    def log_csv(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(NN):\n",
    "    def print(self):\n",
    "        print(self.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ANN(layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu\n"
     ]
    }
   ],
   "source": [
    "b.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {} # model will be returned as dictionary\n",
    "losses = [] # initialize loss array\n",
    "train_accs = [] # initialize training accuracy array\n",
    "val_accs = [] # initialize validation accuracy array\n",
    "params = global_param_init(layer_dims, weight_init) # initialize weights\n",
    "t = tqdm.trange(num_steps) # tqdm object\n",
    "best_epoch = num_steps # initialize best epoch value\n",
    "\n",
    "# Training loop\n",
    "for i in t:\n",
    "    probs, caches = model_forward(X_train, params, activation, dropout_rate) # forward propagation\n",
    "    loss = cat_xentropy_loss(probs, y_train) # calculate loss\n",
    "    grads = model_backward(probs, y_train, caches, activation, dropout_rate) # error backpropagation\n",
    "    params = update_params(params, grads, learning_rate) # weight updates\n",
    "    train_acc = predict(X_train, y_train, params, activation) # training accuracy\n",
    "    val_acc = predict(X_test, y_test, params, activation) # testing accuracy\n",
    "    t.set_postfix(loss=float(loss), train_acc=train_acc, val_acc=val_acc) # tqdm printing\n",
    "    losses.append(loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    # Record training logs\n",
    "    if early_stopping and val_acc > 0.99:\n",
    "        best_epoch = i\n",
    "        print()\n",
    "        print('Early Stopping at Epoch: {}'.format(i))      \n",
    "        break # stop training if maximum accuracy is achieved\n",
    "print('Training Finished')\n",
    "print('Training Accuracy Score: {:.2f}%'.format(train_acc*100))\n",
    "print('Validation Accuracy Score: {:.2f}%'.format(val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = {} # model will be returned as dictionary\n",
    "    losses = [] # initialize loss array\n",
    "    train_accs = [] # initialize training accuracy array\n",
    "    val_accs = [] # initialize validation accuracy array\n",
    "    params = global_param_init(layer_dims, weight_init) # initialize weights\n",
    "    t = tqdm.trange(num_steps) # tqdm object\n",
    "    best_epoch = num_steps # initialize best epoch value\n",
    "\n",
    "    # Training loop\n",
    "    for i in t:\n",
    "        probs, caches = model_forward(X_train, params, activation, dropout_rate) # forward propagation\n",
    "        loss = cat_xentropy_loss(probs, y_train) # calculate loss\n",
    "        grads = model_backward(probs, y_train, caches, activation, dropout_rate) # error backpropagation\n",
    "        params = update_params(params, grads, learning_rate) # weight updates\n",
    "        train_acc = predict(X_train, y_train, params, activation) # training accuracy\n",
    "        val_acc = predict(X_test, y_test, params, activation) # testing accuracy\n",
    "        t.set_postfix(loss=float(loss), train_acc=train_acc, val_acc=val_acc) # tqdm printing\n",
    "        losses.append(loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # Record training logs\n",
    "        if early_stopping and val_acc > 0.99:\n",
    "            best_epoch = i\n",
    "            print()\n",
    "            print('Early Stopping at Epoch: {}'.format(i))      \n",
    "            break # stop training if maximum accuracy is achieved\n",
    "    print('Training Finished')\n",
    "    print('Training Accuracy Score: {:.2f}%'.format(train_acc*100))\n",
    "    print('Validation Accuracy Score: {:.2f}%'.format(val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = NN(layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0495"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'W1': array([[-0.10243756,  0.36800669,  0.18942226,  0.08055432, -0.2808596 ,\n",
       "         -0.2808793 , -0.36082322,  0.29898157,  0.08256006,  0.16989055,\n",
       "         -0.39144112,  0.38367979,  0.27143828, -0.23487413, -0.25978883,\n",
       "         -0.25849914, -0.15983554,  0.02021354, -0.05556666, -0.17046069,\n",
       "          0.09132751, -0.29435203, -0.16971318, -0.1091151 , -0.03586871,\n",
       "          0.2328452 , -0.24521533,  0.01162237,  0.07545618, -0.37032169,\n",
       "          0.08781   , -0.26901593],\n",
       "        [-0.35513389,  0.36651351,  0.38018696,  0.25180538, -0.15953219,\n",
       "         -0.32849934,  0.15042564, -0.04886528, -0.30860449, -0.00393804,\n",
       "         -0.38017018,  0.33420871, -0.19695532,  0.13269889, -0.15373726,\n",
       "          0.01638547,  0.03813878, -0.25731526,  0.38341424,  0.22464501,\n",
       "          0.35884938,  0.32237518,  0.079935  ,  0.34445887, -0.33599447,\n",
       "         -0.24822895, -0.37132036, -0.14261719, -0.09089461, -0.18669273,\n",
       "          0.26841305, -0.11696042],\n",
       "        [-0.17886622,  0.03486121, -0.29318414,  0.2467428 , -0.34737794,\n",
       "          0.39754152,  0.22228692, -0.24599762, -0.4037395 ,  0.25757318,\n",
       "          0.16889831,  0.18698357,  0.22149131, -0.34779109, -0.11556225,\n",
       "         -0.3136416 ,  0.29647271,  0.1006725 , -0.13807118, -0.35635311,\n",
       "         -0.15433229, -0.14273722,  0.18747266,  0.11231521,  0.31615788,\n",
       "         -0.02268642, -0.3106    ,  0.17411364,  0.2129301 ,  0.05003262,\n",
       "          0.22124378, -0.00506587],\n",
       "        [ 0.01856128, -0.05916251, -0.38749366, -0.32015531, -0.38258647,\n",
       "          0.11137863, -0.15157771,  0.00699794,  0.33277663, -0.20470204,\n",
       "         -0.07317204,  0.20865663, -0.22143537, -0.34539446, -0.17166722,\n",
       "         -0.27661166,  0.35084666,  0.25157924,  0.10892371,  0.3032963 ,\n",
       "          0.24794721, -0.25591448,  0.32052308,  0.03212281,  0.25102384,\n",
       "          0.32340719, -0.14859954, -0.31839127, -0.22214001, -0.05951624,\n",
       "          0.25965797,  0.29453529]]),\n",
       " 'b2': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'W2': array([[-0.24652393,  0.00537365, -0.0412945 , ..., -0.12907385,\n",
       "         -0.20344862,  0.19860788],\n",
       "        [ 0.20020903,  0.06655073, -0.0804851 , ..., -0.091539  ,\n",
       "         -0.16525363,  0.02840063],\n",
       "        [ 0.21807739,  0.0980149 ,  0.03503059, ..., -0.16335284,\n",
       "         -0.17178148, -0.12487855],\n",
       "        ...,\n",
       "        [-0.18326001, -0.20969924,  0.11396965, ...,  0.11140715,\n",
       "         -0.21616582,  0.10391755],\n",
       "        [ 0.02176911, -0.20913733, -0.02084968, ...,  0.24176074,\n",
       "         -0.17964424, -0.14899217],\n",
       "        [-0.15788758,  0.19699486,  0.07714628, ...,  0.17786863,\n",
       "         -0.13641022,  0.16852056]]),\n",
       " 'b3': array([[0., 0., 0.]]),\n",
       " 'W3': array([[-0.1321045 ,  0.08551562,  0.11620021],\n",
       "        [ 0.00757394, -0.11652265, -0.17198395],\n",
       "        [-0.27938879, -0.11733918,  0.09166879],\n",
       "        [ 0.26232789,  0.22216796,  0.15924126],\n",
       "        [ 0.17263735,  0.09874439, -0.14346966],\n",
       "        [ 0.24370851,  0.10218425,  0.03617406],\n",
       "        [-0.23282498, -0.03168755, -0.0237258 ],\n",
       "        [ 0.21819336,  0.02791922, -0.07158094],\n",
       "        [ 0.28536767, -0.23298591, -0.0463552 ],\n",
       "        [-0.2741008 ,  0.14358396,  0.25022116],\n",
       "        [-0.13164889,  0.2144695 , -0.12435855],\n",
       "        [ 0.24584824,  0.15199762,  0.18248841],\n",
       "        [-0.28847578,  0.27699459,  0.13566386],\n",
       "        [-0.11685756,  0.19714455, -0.13076079],\n",
       "        [ 0.22309526, -0.23187281,  0.1219072 ],\n",
       "        [ 0.02435827, -0.24147638, -0.15447799],\n",
       "        [-0.29182892, -0.01869255, -0.11894385],\n",
       "        [ 0.05886751, -0.12135431, -0.11974953],\n",
       "        [ 0.14555249, -0.27043904,  0.24113491],\n",
       "        [ 0.2108319 ,  0.10043204,  0.05579363],\n",
       "        [ 0.23479524, -0.18833165, -0.25198931],\n",
       "        [-0.15590462,  0.17630675, -0.27850245],\n",
       "        [ 0.04956035,  0.29652213,  0.2128861 ],\n",
       "        [ 0.01283537, -0.26116336,  0.1983289 ],\n",
       "        [ 0.05923919, -0.23046476, -0.24307869],\n",
       "        [ 0.24516395,  0.10126731,  0.19707999],\n",
       "        [ 0.22682099,  0.04295615,  0.01044174],\n",
       "        [-0.04163958, -0.10955849, -0.03914468],\n",
       "        [ 0.16391851,  0.06100174,  0.23492739],\n",
       "        [-0.03388738,  0.06409371,  0.07858831],\n",
       "        [ 0.05488127,  0.12127745, -0.15714754],\n",
       "        [ 0.00739979, -0.23687367, -0.06912062],\n",
       "        [-0.00738133,  0.09110707,  0.26964536],\n",
       "        [ 0.06024023,  0.14579232,  0.00375027],\n",
       "        [ 0.08026201, -0.25679947, -0.14699801],\n",
       "        [-0.0826817 , -0.01646288, -0.27193177],\n",
       "        [-0.21544759, -0.13357789,  0.28221496],\n",
       "        [-0.10093976, -0.01074853, -0.18188722],\n",
       "        [ 0.06630249, -0.13126232, -0.17536658],\n",
       "        [ 0.00991881, -0.29595755, -0.29466583],\n",
       "        [-0.16813889, -0.27727486, -0.23459878],\n",
       "        [-0.0964428 ,  0.18109922,  0.04312152],\n",
       "        [ 0.0075817 , -0.1235981 ,  0.25840702],\n",
       "        [-0.06163707, -0.24712729,  0.07006497],\n",
       "        [-0.23111988, -0.09263482,  0.00443609],\n",
       "        [ 0.22397428, -0.00386241,  0.121053  ],\n",
       "        [ 0.29495364, -0.22055583, -0.13482444],\n",
       "        [-0.06309681, -0.04678627, -0.05325438],\n",
       "        [ 0.24395747,  0.12809974,  0.06458188],\n",
       "        [-0.1140915 ,  0.19379036,  0.27235627],\n",
       "        [ 0.19223897, -0.29831608,  0.08163698],\n",
       "        [-0.26864799, -0.14507344, -0.26362693],\n",
       "        [ 0.06212216,  0.11167511, -0.23073131],\n",
       "        [-0.06952308, -0.02619269, -0.0783725 ],\n",
       "        [-0.22681866, -0.0485102 ,  0.15033147],\n",
       "        [-0.25675432, -0.25126461, -0.0869202 ],\n",
       "        [ 0.26437531,  0.10089167,  0.10693497],\n",
       "        [-0.08264166,  0.05605651, -0.29318923],\n",
       "        [ 0.08145425,  0.24735455,  0.06737585],\n",
       "        [ 0.2236607 ,  0.13404913, -0.22709791],\n",
       "        [ 0.24087056, -0.25948579,  0.02032972],\n",
       "        [-0.21418294, -0.29224633, -0.04666293],\n",
       "        [-0.12266928, -0.00837878,  0.04620456],\n",
       "        [-0.27307472, -0.22563464,  0.0350978 ]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train, X_test, y_test, layer_dims, activation='sigmoid', weight_init='glorot_uniform', dropout_rate=None, learning_rate=0.0495, num_steps=3000, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNet:\n",
    "\n",
    "  def __init__(self, layer_dims):\n",
    "    self.W = None\n",
    "\n",
    "  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: D x N array of training data. Each training point is a D-dimensional\n",
    "         column.\n",
    "    - y: 1-dimensional array of length N with labels 0...K-1, for K classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    dim, num_train = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    if self.W is None:\n",
    "      # lazily initialize W\n",
    "      self.W = np.random.randn(num_classes, dim) * 0.001\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "    for it in xrange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      batch_mask = np.random.choice(num_train, batch_size)\n",
    "      X_batch = X[:,batch_mask]\n",
    "      y_batch = y[batch_mask]\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # perform parameter update\n",
    "      step = -learning_rate * grad\n",
    "      self.W += step\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print 'iteration %d / %d: loss %f' % (it, num_iters, loss)\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict labels for\n",
    "    data points.\n",
    "\n",
    "    Inputs:\n",
    "    - X: D x N array of training data. Each column is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[1])\n",
    "    scores = self.W.dot(X)\n",
    "    y_pred = np.argmax(scores, axis=0) # top scoring class\n",
    "    return y_pred\n",
    "  \n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: D x N array of data; each column is a data point.\n",
    "    - y_batch: 1-dimensional array of length N with labels 0...K-1, for K classes.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an array of the same shape as W\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class LinearSVM(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    return svm_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Structured SVM loss function, vectorized implementation.\n",
    "\n",
    "  Inputs and outputs are the same as svm_loss_naive.\n",
    "  \"\"\"\n",
    "  loss = 0.0\n",
    "  dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "  D, num_train = X.shape\n",
    "  scores = W.dot(X)\n",
    "  correct_class_scores = scores[y, range(num_train)]\n",
    "  margins = np.maximum(0, scores - correct_class_scores + 1.0)\n",
    "  margins[y, range(num_train)] = 0\n",
    "\n",
    "  loss_cost = np.sum(margins) / num_train\n",
    "  loss_reg = 0.5 * reg * np.sum(W * W)\n",
    "  loss = loss_cost + loss_reg\n",
    "  num_pos = np.sum(margins > 0, axis=0) # number of positive losses\n",
    "\n",
    "  dscores = np.zeros(scores.shape)\n",
    "  dscores[margins > 0] = 1\n",
    "  dscores[y, range(num_train)] = -num_pos\n",
    "\n",
    "  dW = dscores.dot(X.T) / num_train + reg * W\n",
    "\n",
    "  return loss, dW\n",
    "\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, vectorized version.\n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "\n",
    "  D, num_train = X.shape\n",
    "  scores = W.dot(X) # C x N\n",
    "\n",
    "  scores -= np.max(scores, axis = 0)\n",
    "  p = np.exp(scores)\n",
    "  p /= np.sum(p, axis = 0)\n",
    "\n",
    "  loss_cost = -np.sum(np.log(p[y, range(y.size)])) / num_train\n",
    "  loss_reg = 0.5 * reg * np.sum(W * W)\n",
    "  loss = loss_cost + loss_reg\n",
    "\n",
    "  dscores = p\n",
    "  dscores[y, range(y.size)] -= 1.0\n",
    "  dW = dscores.dot(X.T) / num_train + reg * W\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
