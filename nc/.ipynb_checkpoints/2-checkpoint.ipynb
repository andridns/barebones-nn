{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnets import weight_init, layers\n",
    "from dnets.utils import init_model, global_param_init\n",
    "from dnets.preprocessing import train_test_split\n",
    "from dnets.layers import model_forward, model_backward\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [4, 32, 64, 4]\n",
    "model = {}\n",
    "model = init_model(layer_dims=layer_dims,\n",
    "                          activation='RELU', weight_init='GLOROT_UNiform',\n",
    "                          dropout_rate=None, learning_rate=0.001, num_steps=3000, \n",
    "                          early_stopping=True)\n",
    "model = global_param_init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 (1, 32)\n",
      "W1 (4, 32)\n",
      "b2 (1, 64)\n",
      "W2 (32, 64)\n",
      "b3 (1, 4)\n",
      "W3 (64, 4)\n"
     ]
    }
   ],
   "source": [
    "for k, v in model['var'].items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c07f81f0dca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tqdm object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;31m# initialize best epoch value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_steps' is not defined"
     ]
    }
   ],
   "source": [
    "n\n",
    "\n",
    "\n",
    "t = tqdm.trange(num_steps) # tqdm object\n",
    "best_epoch = num_steps # initialize best epoch value\n",
    "\n",
    "# Training loop\n",
    "for i in t:\n",
    "    probs, caches = model_forward(X_train, params, activation, dropout_rate) # forward propagation\n",
    "    loss = cat_xentropy_loss(probs, y_train) # calculate loss\n",
    "    grads = model_backward(probs, y_train, caches, activation, dropout_rate) # error backpropagation\n",
    "    params = update_params(params, grads, learning_rate) # weight updates\n",
    "    train_acc = predict(X_train, y_train, params, activation) # training accuracy\n",
    "    val_acc = predict(X_test, y_test, params, activation) # testing accuracy\n",
    "    t.set_postfix(loss=float(loss), train_acc=train_acc, val_acc=val_acc) # tqdm printing\n",
    "    losses.append(loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    # Record training logs\n",
    "    if early_stopping and val_acc > 0.99:\n",
    "        best_epoch = i\n",
    "        print()\n",
    "        print('Early Stopping at Epoch: {}'.format(i))      \n",
    "        break # stop training if maximum accuracy is achieved\n",
    "print('Training Finished')\n",
    "print('Training Accuracy Score: {:.2f}%'.format(train_acc*100))\n",
    "print('Validation Accuracy Score: {:.2f}%'.format(val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_forward(A, W, b):\n",
    "    \"\"\"Dense (fully-connected) layer forward propagation.\"\"\"\n",
    "    cache = (A, W, b)\n",
    "    Z = A.dot(W) + b\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = model['activation']\n",
    "for l in range(1,len(layer_dims)):\n",
    "    Z, linear_cache = dense_forward(X_train, model['var']['W1'], model['var']['b1'])\n",
    "    A, cache = getattr(layers, f'{act}')(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"Sigmoid forward propagation layer with cache returned.\"\"\"  \n",
    "    cache = Z.copy()\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['caches'] = np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pop('cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward(A_prev, W, b, activation, dropout_rate=None):\n",
    "    \"\"\"Forward propagation of a dense layer with an activation function of choice.\n",
    "\n",
    "    Args:\n",
    "        A_prev: activation matrix from previous layer\n",
    "        W: weight matrix of current layer \n",
    "        b: bias vector of current layer\n",
    "        activation (str): activation function, e.g. \"sigmoid\", \"relu\"\n",
    "        dropout_rate (float): probability of randomly setting activations to zero\n",
    "\n",
    "    Returns:\n",
    "        A: output of the activation function\n",
    "        cache: tuple of values needed for backpropagation\n",
    "    \"\"\"\n",
    "    if activation.lower() == \"sigmoid\":\n",
    "        Z, linear_cache = dense_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        if dropout_rate: # apply dropout if not None\n",
    "            A, dropout_mask = dropout(A, dropout_rate)\n",
    "            cache = (linear_cache, activation_cache, dropout_mask)\n",
    "    elif activation.lower() == \"relu\":\n",
    "        Z, linear_cache = dense_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        if dropout_rate: # apply dropout if not None\n",
    "            A, dropout_mask = dropout(A, dropout_rate)\n",
    "            cache = (linear_cache, activation_cache, dropout_mask)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['caches'] = np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get('lol') is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['caches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_forward(A, W, b):\n",
    "    \"\"\"Dense (fully-connected) layer forward propagation.\"\"\"\n",
    "    cache = (A, W, b)\n",
    "    Z = A.dot(W) + b\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, cache = dense_forward(X_train, model['var']['W1'], model['var']['b1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"ReLU forward propagation layer with cache returned.\"\"\"\n",
    "    cache = Z.copy()\n",
    "    A = np.maximum(0,Z)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(weight_init, 'glorot_uniform')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "model = initialize_model(layer_dims=layer_dims,\n",
    "                          activation='RELU', weight_init='GLOROT_UNiform',\n",
    "                          dropout_rate=None, learning_rate=0.001, num_steps=3000, \n",
    "                          early_stopping=True)\n",
    "model = global_param_init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in model['var'].items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from layers import model_forward, model_backward\n",
    "from utils import predict, global_param_init, update_params, plot, log_csv\n",
    "from metrics import cat_xentropy_loss\n",
    "from preprocessing import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "layer_dims = [4, 32, 64, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(**kwargs):\n",
    "    for k,v in kwargs.items():\n",
    "        if type(v) == str:\n",
    "            model[k] = v.lower()\n",
    "        else:\n",
    "            model[k] = v\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "model = initialize_model(layer_dims=layer_dims,\n",
    "                          activation='RELU', weight_init='GLOROT_UNiform',\n",
    "                          dropout_rate=None, learning_rate=0.001, num_steps=3000, \n",
    "                          early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers\n",
    "act = 'relu'\n",
    "getattr(layers, f'{act}')(np.arange(-10,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_train, y_test, num_steps):\n",
    "    losses = [] # initialize loss array\n",
    "    train_accs = [] # initialize training accuracy array\n",
    "    val_accs = [] # initialize validation accuracy array\n",
    "    t = tqdm.trange(num_steps) # tqdm object\n",
    "    best_epoch = num_steps # initialize best epoch value\n",
    "    # Training loop\n",
    "    for i in t:\n",
    "        probs, caches = model_forward(X_train, params, activation, dropout_rate) # forward propagation\n",
    "        loss = cat_xentropy_loss(probs, y_train) # calculate loss\n",
    "        grads = model_backward(probs, y_train, caches, activation, dropout_rate) # error backpropagation\n",
    "        params = update_params(params, grads, learning_rate) # weight updates\n",
    "        train_acc = predict(X_train, y_train, params, activation) # training accuracy\n",
    "        val_acc = predict(X_test, y_test, params, activation) # testing accuracy\n",
    "        t.set_postfix(loss=float(loss), train_acc=train_acc, val_acc=val_acc) # tqdm printing\n",
    "        losses.append(loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # Record training logs\n",
    "        if early_stopping and val_acc > 0.99:\n",
    "            best_epoch = i\n",
    "            print()\n",
    "            print('Early Stopping at Epoch: {}'.format(i))      \n",
    "            break # stop training if maximum accuracy is achieved\n",
    "    print('Training Finished')\n",
    "    print('Training Accuracy Score: {:.2f}%'.format(train_acc*100))\n",
    "    print('Validation Accuracy Score: {:.2f}%'.format(val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    a = model['lol'] if 'lol' in model else 42\n",
    "    print(model['activation'])\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [4, 32, 64, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "model = initialize_model(layer_dims=layer_dims,\n",
    "                          activation='relu', weight_init='glorot_uniform',\n",
    "                          dropout_rate=None, learning_rate=0.001, num_steps=3000, \n",
    "                          early_stopping=True, lol=1000)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'layer_dims' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['params'] = global_param_init(layer_dims, weight_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
